#!/usr/bin/env python

import gc, time, commands, os, sys, socket
import numpy as np
from osgeo import gdal
from optparse import OptionParser
from cStringIO import StringIO
from frange import *


#TODO: Integrate frange with metadata generation
#      Report status to xmlrpc service/mongodb/mysql/whatever
#      Report errors to same.
#      Modify hadoop driver to use this script
#      Test this script with hadoop
#      Create new collection creation script to extract the browse & index
#      and put them where they belong
#      Figure out if you really need that crazy directory structure for
#      collection images or if you can just put them all in one directory.
#      Include src files in distro
#      change hadoop script + cron script + helper script
#      Change webpage + db to allow options to be captured

#==============================================================
# recursively removes zeros off the supplied string and returns
# the cleansed value
#==============================================================
def stripZeros(value):
    
    while value.startswith('0'):
        value = value[1:len(value)]
        
    return value

#==============================================================
#Cooresponding path for this scene
#==============================================================
def getPath(scene_name):
    return stripZeros(scene_name[3:6])

#==============================================================
#Corresponding row for this scene
#==============================================================
def getRow(scene_name):
    return stripZeros(scene_name[6:9])

#==============================================================
#Scene collection year
#==============================================================
def getYear(scene_name):
    return scene_name[9:13]

#==============================================================
#Scene collection julian date
#==============================================================
def getDoy(scene_name):
    return scene_name[13:16]

#==============================================================
#return scene sensor
#==============================================================
def getSensor(scene_name):
    if scene_name[0:3] =='LT5':
        return 'tm'
    elif scene_name[0:3] == 'LE7':
        return 'etm'

#==============================================================
#return xy coordinates for the given line from gdalinfo
#==============================================================
def getXY(value):
    '''Returns the xy coordinates for the given line from gdalinfo'''
    parts = value.split('(')    
    p = parts[1].split(')')
    p = p[0].split(',')
    return (p[1].strip(),p[0].strip())


#==============================================================
#parse gdal coordinates from gdalinfo
#==============================================================
def parseGdalInfo(gdalFile, debug=False):

    cmd = "gdalinfo %s |grep \(" % (gdalFile)
    
    status,output = commands.getstatusoutput(cmd)
    contents = output

    if debug:
        print ("Parse GDAL Info")
        print contents

    results = dict()
        
    lines = contents.split('\n')
    for l in lines:
        if l.startswith('Upper Left'):
            results['browse.ul'] = getXY(l)
        elif l.startswith('Lower Left'):
            results['browse.ll'] = getXY(l)
        elif l.startswith('Upper Right'):
            results['browse.ur'] = getXY(l)
        elif l.startswith('Lower Right'):
            results['browse.lr'] = getXY(l)
            

    return results

#==============================================================
#return scene metadata as a dictionary
#==============================================================
def getMetaData(work_dir, debug=False):
        #find the metadata file
        mtl_file = ''
        items = os.listdir(work_dir)
        for i in items:
            if not i.startswith('lnd') and (i.find('_MTL') > 0) and not (i.find('old') > 0):
                mtl_file = i
                print ("Located MTL file:%s" % mtl_file)
                break

        if mtl_file == '':
            print("Could not locate the landsat MTL file in %s" % work_dir)
            return None
        
        
        current_dir = os.getcwd()
        os.chdir(work_dir)
        f = open(mtl_file, 'r')
        data = f.readlines()
        f.close()
    
        #this will fix the problem ledaps has with binary characters at the end
        #of some of the gls metadata files
        length = len(data)
        buff = StringIO()
    
        count = 1
        for d in data:
            if count < length:
                buff.write(d)
                count = count + 1
    
        #fix the stupid error where the metadata txt file is named TIF
        mtl_file = mtl_file.replace('.TIF', '.txt')
            
        f = open(mtl_file, 'w+')
        fixedmeta = buff.getvalue()
        f.write(fixedmeta)
        f.flush()
        f.close()
        #print buffer.getvalue()
        buff.close()
        os.chdir(current_dir)
        
        #now we are going to read all the metadata into the context{} as
        #a dictionary.  Needed later for generating the solr index et. al.
        metadata = {}

        fixedmeta = fixedmeta.split('\n')
        for line in fixedmeta:
            line = line.strip()
            #print ('Meta line:%s' % line)
            if not line.startswith('END') and not line.startswith('GROUP'):
                parts = line.split('=')
                if len(parts) == 2:
                    metadata[parts[0].strip()] = parts[1].strip().replace('"', '')

        metadata['mtl_file'] = mtl_file
         
        return metadata

#==============================================================
#expands an HDF to Geotiff bands
#==============================================================
def convertHDFToGTiff(hdf_file, target_filename):
    status = 0
    try:
        cmd = ('gdal_translate -a_nodata -9999 -a_nodata 12000 -of GTIFF -sds %s %s') % (hdf_file, target_filename)
        status,output = commands.getstatusoutput(cmd)
    except Exception,e:
        print e
        return -1
    
    return status
        

#==============================================================
#create a browse image for the product
#==============================================================
def makeBrowse(work_dir,metadata, scene_name,debug=False):
    print("Executing MakeBrowse()")
                    
    try:
        extrasdir = work_dir #os.path.join(work_dir, 'extras')
        output_file = "%s-sr-browse.tif" % scene_name
        output_file = os.path.join(extrasdir, output_file)
        
        if not os.path.exists(extrasdir):
            os.makedirs(extrasdir)

        convertHDFToGTiff("%s/lndsr*hdf" % work_dir, "%s/out.tiff" % extrasdir)
        
        cmds = []
        #cmds.append(('gdal_translate -of GTIFF -sds %s/lndsr*hdf %s/out.tiff') % (context['work.dir'], browsedir))
        cmds.append(('gdal_translate -ot Byte -scale 0 10000 0 255 -of GTIFF %s/out.tiff5 %s/browse.tiff5') % (extrasdir, extrasdir))
        cmds.append(('gdal_translate -ot Byte -scale 0 10000 0 255 -of GTIFF %s/out.tiff4 %s/browse.tiff4') % (extrasdir,extrasdir))
        cmds.append(('gdal_translate -ot Byte -scale 0 10000 0 255 -of GTIFF %s/out.tiff3 %s/browse.tiff3') % (extrasdir,extrasdir))
        cmds.append(('gdal_merge_simple -in %s/browse.tiff5 -in %s/browse.tiff4 -in %s/browse.tiff3 -out %s/final.tif') % (extrasdir,extrasdir,extrasdir,extrasdir))

        #deproject into geographic
        cmds.append(('gdalwarp -dstalpha -srcnodata 0 -t_srs EPSG:4326 %s/final.tif %s/warped.tif') % (extrasdir,extrasdir))

        #resize and rename
        cmds.append(('gdal_translate -co COMPRESS=DEFLATE -co PREDICTOR=2 -outsize 50%% 50%% -a_nodata -9999 -of GTIFF %s/warped.tif %s') % (extrasdir, output_file))

        #cleanup        
        cmds.append(('rm -rf %s/warped.tif') % (extrasdir))
        cmds.append(('rm -rf %s/*tiff*') % (extrasdir))
        cmds.append(('rm -rf %s/*out*') % (extrasdir))
        cmds.append(('rm -rf %s/final.tif') % (extrasdir))
                
        for cmd in cmds:
            if debug:
                print "Running:%s" % cmd
            status,output = commands.getstatusoutput(cmd)
            if status != 0:
                print ("Error occurred running:%s" % cmd)
                return status
            
        #add the browse cornerpoints to the context here
        #need to pull these from the level 1 metadata (IF it's already in longlat that is) instead so we have actual data cornerpoints instead of
        #scene cornerpoints
        coords = parseGdalInfo(output_file)
        metadata['BROWSE_UL_CORNER_LAT'] = coords['browse.ul'][0]
        metadata['BROWSE_UL_CORNER_LON'] = coords['browse.ul'][1]
        metadata['BROWSE_UR_CORNER_LAT'] = coords['browse.ur'][0]
        metadata['BROWSE_UR_CORNER_LON'] = coords['browse.ur'][1]
        metadata['BROWSE_LL_CORNER_LAT'] = coords['browse.ll'][0]
        metadata['BROWSE_LL_CORNER_LON'] = coords['browse.ll'][1]
        metadata['BROWSE_LR_CORNER_LAT'] = coords['browse.lr'][0]
        metadata['BROWSE_LR_CORNER_LON'] = coords['browse.lr'][1]          
                                    
        print("MakeBrowse() complete...")
    except Exception,e:
        print e
        return -1
    finally:
        pass
    return 0
                
    

#==============================================================
#create a solr index file for the current scene
#TODO: include calls to frange here to build full index of points
#across scene
#==============================================================
def makeSolrIndex(metadata, scene_name, work_dir, collection_name,debug=False):
    try:
        print("Executing MakeSolrIndex() for %s" % scene_name)

        header =  "sceneid;acquisitionDate;sensor;path;row;"
        header += "upperLeftCornerLatLong;upperRightCornerLatLong;"
        header += "lowerLeftCornerLatLong;lowerRightCornerLatLong;"
        header += "sunElevation;"
        header += "sunAzimuth;groundStation;collection"
            
        if debug:
            print metadata
            
        sceneid = scene_name

        acquisitionDate = None
    
        if metadata.has_key('DATE_ACQUIRED'):
            acquisitionDate = metadata['DATE_ACQUIRED'] + "T00:00:01Z"
        else:
            acquisitionDate = metadata['ACQUISITION_DATE'] + "T00:00:01Z"

        sensor = metadata['SENSOR_ID']
        path = metadata['WRS_PATH']
            
        row = None
        #this is a fix for the changes to landsat metadata... currently have mixed versions on the cache
        if metadata.has_key("WRS_ROW"):
            row = metadata['WRS_ROW']
        else:
            row = metadata['STARTING_ROW']
                          
   
        upper_left_LL = "%s,%s" % (metadata['BROWSE_UL_CORNER_LAT'], metadata['BROWSE_UL_CORNER_LON'])
        upper_right_LL = "%s,%s" % (metadata['BROWSE_UR_CORNER_LAT'], metadata['BROWSE_UR_CORNER_LON'])
        lower_left_LL = "%s,%s" % (metadata['BROWSE_LL_CORNER_LAT'], metadata['BROWSE_LL_CORNER_LON'])
        lower_right_LL = "%s,%s" % (metadata['BROWSE_LR_CORNER_LAT'], metadata['BROWSE_LR_CORNER_LON'])

        sun_elevation = metadata['SUN_ELEVATION']
        sun_azimuth = metadata['SUN_AZIMUTH']
        ground_station = metadata['STATION_ID']
        collection = collection_name
            
        index_string = ("%s;%s;%s;%s;%s;%s;%s;%s;%s;%s;%s;%s;%s;%s") % (
            sceneid, acquisitionDate,sensor,path,row,
            upper_left_LL, upper_right_LL,lower_left_LL, lower_right_LL,
            sun_elevation, sun_azimuth, ground_station, collection,'EOL'
            )
            
                    
        index_file = ("%s-index.csv") % scene_name
        index_file = os.path.join(work_dir, index_file)
        f = open(index_file, 'w')
        f.write(header + '\n')
        f.write(index_string + '\n')
        f.close()
    except Exception,e:
        print e
        return -1
    
    print("MakeSolrIndex() complete...")
    return 0

#==============================================================
#create NDVI product for current scene
#==============================================================
def makeNDVI(work_directory,scene_name,debug=False):
    print("Executing NDVI()")
                       
    try:
        ndviDir = "%s" % work_directory
        ndvi_output_file = "%s-sr-ndvi.tif" % scene_name
        ndvi_output_file = os.path.join(ndviDir, ndvi_output_file)
        
        #start with a clean slate
        if not os.path.exists(ndviDir):
            os.makedirs(ndviDir)

        status = convertHDFToGTiff("%s/lndsr*hdf" % work_directory, "%s/out.tiff" % ndviDir)
        if status != 0:
            print ("Error converting lndsr to Geotiff")
            return status

        gc.collect()
            
        # load the proper geotiff bands into GDAL 
        red_file = ("%s/out.tiff3") % (ndviDir)
        in_ds = gdal.Open(red_file) 
        red = in_ds.ReadAsArray()
        geo = in_ds.GetGeoTransform()  
        proj = in_ds.GetProjection()   
        shape = red.shape          
        in_ds = None

        nir_file = ("%s/out.tiff4") % (ndviDir)
        in_ds = gdal.Open(nir_file)
        nir = in_ds.ReadAsArray()
        in_ds = None


        # NDVI = (nearInfrared - red) / (nearInfrared + red)
        nir = np.array(nir, dtype = float)  # change the array data type from integer to float to allow decimals
        red = np.array(red, dtype = float)

        np.seterr(divide='ignore')
                
        numerator = np.subtract(nir, red) 
        denominator = np.add(nir, red)
        nir = None
        red = None
        gc.collect()

        ndvi = np.divide(numerator,denominator)
        numerator = None
        denominator = None
        gc.collect()

        #put this into 10000 range
        ndvi = np.multiply(ndvi, 10000)
        gc.collect()
                
        #set all negative values to 0
        np.putmask(ndvi, ndvi < 0, 0)
                
        #set all values greater than 10000 to 10000
        np.putmask(ndvi, ndvi > 10000, 10000)
                
        driver = gdal.GetDriverByName('GTiff')

      
        ndvifile = ('%s/ndvi.tif') % (ndviDir)
        dst_ds = driver.Create( ndvifile, shape[1], shape[0], 1, gdal.GDT_Float32)

        # here we set the variable dst_ds with 
        # destination filename, number of columns and rows
        # 1 is the number of bands we will write out
        # gdal.GDT_Float32 is the data type - decimals
        dst_ds.SetGeoTransform(geo)
        dst_ds.SetProjection(proj) 
        dst_ds.GetRasterBand(1).WriteArray(ndvi)  
        stat = dst_ds.GetRasterBand(1).GetStatistics(1,1)
        dst_ds.GetRasterBand(1).SetStatistics(stat[0], stat[1], stat[2], stat[3])
        dst_ds = None

        gc.collect()

        in_ds = None
        dst_ds = None

        cmd = ('gdal_translate -ot UInt16 -scale 0 10000 0 10000 -of GTiff %s %s') % (ndvifile, ndvi_output_file)
        status,output = commands.getstatusoutput(cmd)
        if status != 0:
            print ("Error converting ndvi.tif to %s" % ndvi_output_file)
            return status
                
        cmd = ('rm -rf %s/out.tiff* %s/ndvi.tif') % (ndviDir, ndviDir)
        status,output = commands.getstatusoutput(cmd)
    except Exception, e:
        print e
        return -1
    finally:
        gc.collect()

    print ("NDVI() complete...")
    return 0
        

#==============================================================
#returns the station this scene was acquired from
#==============================================================
def getStation(scene_name):
    return scene_name[16:21]



#==============================================================
#Runs the script
#==============================================================
if __name__ == '__main__':
    parser = OptionParser(usage="usage: %prog [options] scenename")
    parser.add_option("--scene",
                      action="store",
                      dest="scene",
                      help="The scene id to process")
    parser.add_option("--ndvi",
                      action="store_true",
                      dest="ndvi_flag",
                      default=False,
                      help="Create ndvi for this scene")
    parser.add_option("--toa",
                      action="store_true",
                      dest="toa_flag",
                      default=False,
                      help="Include Top of Atmosphere in output product")
    parser.add_option("--browse",
                      action="store_true",
                      dest="browse_flag",
                      default=False,
                      help="Create a 3 band browse image for the resulting product")
    parser.add_option("--solr",
                      action="store_true",
                      dest="solr_flag",
                      default=False,
                      help="Create a solr index for the product")
    parser.add_option("--convert_to_tile",
                       action="store_true",
                       dest="tiling_flag",
                       default=False,
                       help="Converts this scene to the EROS tiling scheme") 
    parser.add_option("--output_format",
                       action="store",
                       dest="output_format",
                       default="GTiff",
                       choices=['GTiff', 'JPG', 'PNG', 'HDF4','HDF5',],
                       help="Output format for product.  Defaults to HDF4")
    parser.add_option("--projection",
                       action="store",
                       dest="projection",
                       default="None",
                       choices=['None', 'Geographic', 'UTM', 'Albers', 'Sinusoidal', 'Robinson'],
                       help="Projection for the output product.  Defaults to no reprojection.")
    parser.add_option("--order",
                      action="store",
                      dest="ordernum",
                      help="Includes this scene as part of an order (controls where the file is distributed to)")
    parser.add_option("--collection",
                      action="store",
                      dest="collection_name",
                      help="Includes this scene as part of a collection (controls solr index values)")
    parser.add_option("--source_host",
                      action="store",
                      dest="source_host",
                      default="localhost",
                      help="The host were espa should look for the scene (--scene) to process")
    parser.add_option("--source_directory",
                      action="store",
                      dest="source_directory",
                      help="Directory on source host where the scene is located")
    parser.add_option("--destination_host",
                      action="store",
                      dest="destination_host",
                      default="localhost",
                      help="Host where completed products should be distributed to.")
    parser.add_option("--destination_directory",
                      action="store",
                      dest="destination_directory",
                      help="Directory on the host where the completed product file should be distributed to")
    parser.add_option("--debug",
                      action="store_true",
                      dest="debug",
                      help="Print debug messages while running")
    
    
    (options,args) = parser.parse_args()
    if options.scene is None:
        print ("\n You must specify a scene to process\n")
        parser.print_help()        
        exit(-1)

    if options.ordernum is None and options.collection_name is None and options.destination_directory is None:
        print ("\n Either an ordernumber,collection name or destination directory is required \n")
        parser.print_help()
        exit(-1)

    if options.solr_flag and options.collection_name is None:
        print ("\n A collection name is required when generating a solr index \n")
        parser.print_help()
        exit(-1)

    #WE WON'T RUN ANYTHING WITHOUT HAVING THE WORKING DIRECTORY SET
    if not os.environ.has_key("ESPA_WORK_DIR") or \
    len(os.environ.get("ESPA_WORK_DIR")) < 1:
        print '$ESPA_WORK_DIR not set... exiting'
        sys.exit(1)

    BASE_WORK_DIR = os.environ.get("ESPA_WORK_DIR")
    
    if not os.path.exists(BASE_WORK_DIR):
        print "%s doesn't exist... creating" % BASE_WORK_DIR
        os.makedirs(BASE_WORK_DIR, mode=0755)
        
    #MOVE MOST OF THESE INTO A CONFIG FILE
    base_source_path = '/data/standard_l1t'
    base_output_path = '/data2/LSRD'
    processing_level = 'sr'
    scene = options.scene
    path = getPath(scene)
    row = getRow(scene)
    sensor = getSensor(scene)
    year = getYear(scene)
    doy = getDoy(scene)
    source_host=options.source_host
    destination_host=options.destination_host
    if options.source_directory is not None:
        source_directory = options.source_directory
    else:
        source_directory = ("%s/%s/%s/%s/%s") % (base_source_path, sensor, path, row, year)
    source_filename = "%s.tar.gz" % scene
    source_file = ("%s/%s") % (source_directory,source_filename)
    
    
    product_filename = ("%s-%s") % (scene,processing_level)

    destination_dir = None
    if options.destination_directory is not None:
        destination_dir = options.destination_directory
    elif options.ordernum is not None:
        destination_dir = ("%s/orders/%s") % (base_output_path, options.ordernum)
    else:
        print ("Error determining if scene should be distributed as an order or to a directory")
        sys.exit(-1)
    
    destination_file = ("%s/%s.tar.gz") % (destination_dir,product_filename)
    workdir = ("%s/%s/%s/work") % (BASE_WORK_DIR,processing_level,scene)
    outputdir=("%s/%s/%s/output") % (BASE_WORK_DIR,processing_level,scene)
    localhostname = socket.gethostname()

    #PREPARE LOCAL WORK DIRECTORY
    try:
        if os.path.exists(workdir):
            cmd = "rm -rf %s" % workdir
            status,output = commands.getstatusoutput(cmd)
            if status != 0:
                raise Exception(output)
        os.makedirs(workdir, mode=0755)
    except Exception,e:
        print ("Error cleaning & creating workdir:%s... exiting") % (workdir)
        print e
        sys.exit(1)
    
    #PREPARE LOCAL OUTPUT DIRECTORY
    try:
        if os.path.exists(outputdir):
            cmd = "rm -rf %s" % outputdir
            status,output = commands.getstatusoutput(cmd)
            if status != 0:
                raise Exception(output)
        os.makedirs(outputdir, mode=0755)
    except Exception, e:
        print ("Error cleaning & creating outputdir:%s... exiting") % (outputdir)
        print e
        sys.exit(2)
    
    #TRANSFER THE SOURCE FILE TO THE LOCAL MACHINE
    print ("Transferring %s from %s to %s") % (source_file,source_host,localhostname)  
    cmd = ("scp -C %s:%s %s") % (source_host, source_file, outputdir)
    (status,output) = commands.getstatusoutput(cmd)
    if status != 0:
        print ("Error transferring %s:%s to %s... exiting") % (source_host, source_file, outputdir)
        print output
        sys.exit(3)
    
    #UNPACK THE SOURCE FILE
    print ("Unpacking %s.tar.gz to %s") % (scene, workdir)
    cmd = ("tar --directory %s -xvf %s/%s.tar.gz") % (workdir, outputdir, scene)
    (status,output) = commands.getstatusoutput(cmd)
    if status != 0:
        print ("Error unpacking source file to %s/%s.tar.gz") % (outputdir,scene)
        print output
        sys.exit(4)


    metadata = getMetaData(workdir)
    
    #MAKE THE PRODUCT
    cmd = ("cd %s; do_ledaps.py --metafile %s") % (workdir, metadata['mtl_file'])
    print ("Running LEDAPS against %s") % scene
    status,output = commands.getstatusoutput(cmd)
    if status != 0:
        print ("LEDAPS error detected... exiting")
        print output
        sys.exit(5)
        
    #MAKE BROWSE IMAGE
    if options.browse_flag:
        status = makeBrowse(workdir, metadata, scene)
        if status != 0:
            print ("Error generating browse... exiting")
            sys.exit(6)

    #MAKE NDVI
    if options.ndvi_flag:
        status = makeNDVI(workdir, scene)
        if status != 0:
            print ("Error creating NDVI... exiting")
            sys.exit(7)
    
    #MAKE SOLR INDEX
    if options.solr_flag:
        status = makeSolrIndex(metadata, scene, workdir, options.collection_name)
        if status != 0:
            print ("Error creating solr index... exiting")
            sys.exit(8)

    #DELETE UNNEEDED FILES FROM PRODUCT DIRECTORY
    print("Purging unneeded files from %s") % workdir
    orig_cwd = os.getcwd()
    os.chdir(workdir)
    cmd = "rm -rf *TIF README* LogReport*"
    status,output = commands.getstatusoutput(cmd)
    if status != 0:
        print("Error purging files from %s... exiting") % workdir
        print output
        sys.exit(9)

    #PACKAGE THE PRODUCT FILE
    print ("Packaging completed product to %s/%s.tar.gz") % (outputdir,product_filename)
    cmd = ("tar -cvf %s/%s.tar *") % (outputdir, product_filename)
    status,output = commands.getstatusoutput(cmd)
    os.chdir(orig_cwd)
    if status != 0:
        print ("Error packaging finished product to %s/%s.tar") % (outputdir,product_filename)
        print output
        sys.exit(10)
    
    #COMPRESS THE PRODUCT FILE
    cmd = ("gzip %s/%s.tar") % (outputdir,product_filename)
    status,output = commands.getstatusoutput(cmd)
    if status != 0:
        print ("Error compressing final product file:%s/%s.tar") % (outputdir,product_filename)
        print output
        sys.exit(11)
    
    
    #MAKE DISTRIBUTION DIRECTORIES
    print ("Creating destination directories at %s" % destination_dir)
    cmd = "ssh %s mkdir -p %s" % (destination_host, destination_dir)
    status,output = commands.getstatusoutput(cmd)
    if status != 0:
        print ("Error creating destination directory %s on %s" % (destination_dir,destination_host))
        print output
        sys.exit(12)
    
    #DISTRIBUTE THE PRODUCT FILE
    print ("Transferring %s.tar.gz to %s:%s" % (product_filename,destination_host,destination_file))   
    cmd = "scp -C %s/%s.tar.gz %s:%s" % (outputdir, product_filename, destination_host, destination_file)       
    status,output = commands.getstatusoutput(cmd)
    if status != 0:
        print ("Error transferring %s.tar to %s:%s... exiting" % (product_filename, destination_host,destination_file))
        print output
        sys.exit(13)
    
    #DISTRIBUTE THE SOURCE FILE
    print ("Transferring %s to %s:%s" % (source_filename,destination_host,destination_dir))   
    cmd = "scp -C %s/%s %s:%s" % (outputdir, source_filename, destination_host,destination_dir)
    status,output = commands.getstatusoutput(cmd)
    if status != 0:
        print ("Error transferring %s.tar to %s:%s... exiting" % (source_filename, destination_host,destination_dir))
        print output
        sys.exit(14)    
        
    
    #CLEAN UP THE LOCAL FILESYSTEM
    status,output = commands.getstatusoutput("cd /tmp")
    print ("Cleaning local directories:%s %s" % (outputdir,workdir))
    cmd = "rm -rf %s %s" % (outputdir,workdir)
    status,output = commands.getstatusoutput(cmd)

    if status != 0:
        print("Error cleaning output:%s and work:%s directories... exiting" % (outputdir,workdir))
        print output
        sys.exit(15)
    
    print ("Surface Reflectance Complete")
